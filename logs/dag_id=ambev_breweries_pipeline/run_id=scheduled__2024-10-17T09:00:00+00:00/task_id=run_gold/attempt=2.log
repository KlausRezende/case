[2024-10-18T23:56:59.785+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: ambev_breweries_pipeline.run_gold scheduled__2024-10-17T09:00:00+00:00 [queued]>
[2024-10-18T23:56:59.791+0000] {taskinstance.py:1083} INFO - Dependencies all met for <TaskInstance: ambev_breweries_pipeline.run_gold scheduled__2024-10-17T09:00:00+00:00 [queued]>
[2024-10-18T23:56:59.791+0000] {taskinstance.py:1279} INFO - 
--------------------------------------------------------------------------------
[2024-10-18T23:56:59.791+0000] {taskinstance.py:1280} INFO - Starting attempt 2 of 2
[2024-10-18T23:56:59.792+0000] {taskinstance.py:1281} INFO - 
--------------------------------------------------------------------------------
[2024-10-18T23:56:59.801+0000] {taskinstance.py:1300} INFO - Executing <Task(BashOperator): run_gold> on 2024-10-17 09:00:00+00:00
[2024-10-18T23:56:59.805+0000] {standard_task_runner.py:55} INFO - Started process 19147 to run task
[2024-10-18T23:56:59.807+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'ambev_breweries_pipeline', 'run_gold', 'scheduled__2024-10-17T09:00:00+00:00', '--job-id', '7', '--raw', '--subdir', 'DAGS_FOLDER/dag_brewery.py', '--cfg-path', '/tmp/tmpansnt2pf']
[2024-10-18T23:56:59.808+0000] {standard_task_runner.py:83} INFO - Job 7: Subtask run_gold
[2024-10-18T23:56:59.819+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/***/settings.py:249 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-10-18T23:56:59.834+0000] {logging_mixin.py:137} WARNING - /usr/local/lib/python3.9/site-packages/***/utils/sqlalchemy.py:124 DeprecationWarning: The sql_alchemy_conn option in [core] has been moved to the sql_alchemy_conn option in [database] - the old setting has been used, but please update your config.
[2024-10-18T23:56:59.855+0000] {task_command.py:388} INFO - Running <TaskInstance: ambev_breweries_pipeline.run_gold scheduled__2024-10-17T09:00:00+00:00 [running]> on host 39b0b1bd9d5a
[2024-10-18T23:56:59.899+0000] {taskinstance.py:1507} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=Klaus_Rezende
AIRFLOW_CTX_DAG_ID=ambev_breweries_pipeline
AIRFLOW_CTX_TASK_ID=run_gold
AIRFLOW_CTX_EXECUTION_DATE=2024-10-17T09:00:00+00:00
AIRFLOW_CTX_TRY_NUMBER=2
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-10-17T09:00:00+00:00
[2024-10-18T23:56:59.900+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2024-10-18T23:56:59.900+0000] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'python3 /opt/***/scripts/gold_brewery.py']
[2024-10-18T23:56:59.907+0000] {subprocess.py:86} INFO - Output:
[2024-10-18T23:57:01.277+0000] {subprocess.py:93} INFO - Setting default log level to "WARN".
[2024-10-18T23:57:01.277+0000] {subprocess.py:93} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2024-10-18T23:57:01.449+0000] {subprocess.py:93} INFO - 24/10/18 23:57:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-10-18T23:57:01.882+0000] {subprocess.py:93} INFO - 24/10/18 23:57:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2024-10-18T23:57:01.883+0000] {subprocess.py:93} INFO - 24/10/18 23:57:01 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
[2024-10-18T23:57:04.093+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2024-10-18T23:57:04.094+0000] {subprocess.py:93} INFO -   File "/opt/***/scripts/gold_brewery.py", line 12, in <module>
[2024-10-18T23:57:04.095+0000] {subprocess.py:93} INFO -     df = spark.read.parquet("/opt/***/silver/")
[2024-10-18T23:57:04.095+0000] {subprocess.py:93} INFO -   File "/usr/local/lib/python3.9/site-packages/pyspark/sql/readwriter.py", line 544, in parquet
[2024-10-18T23:57:04.095+0000] {subprocess.py:93} INFO -     return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
[2024-10-18T23:57:04.096+0000] {subprocess.py:93} INFO -   File "/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py", line 1322, in __call__
[2024-10-18T23:57:04.096+0000] {subprocess.py:93} INFO -     return_value = get_return_value(
[2024-10-18T23:57:04.096+0000] {subprocess.py:93} INFO -   File "/usr/local/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py", line 179, in deco
[2024-10-18T23:57:04.096+0000] {subprocess.py:93} INFO -     return f(*a, **kw)
[2024-10-18T23:57:04.097+0000] {subprocess.py:93} INFO -   File "/usr/local/lib/python3.9/site-packages/py4j/protocol.py", line 326, in get_return_value
[2024-10-18T23:57:04.097+0000] {subprocess.py:93} INFO -     raise Py4JJavaError(
[2024-10-18T23:57:04.097+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o25.parquet.
[2024-10-18T23:57:04.097+0000] {subprocess.py:93} INFO - : java.lang.AssertionError: assertion failed: Conflicting directory structures detected. Suspicious paths:
[2024-10-18T23:57:04.098+0000] {subprocess.py:93} INFO - 	file:/opt/***/silver
[2024-10-18T23:57:04.098+0000] {subprocess.py:93} INFO - 	file:/opt/***/silver/column<'current_date()'>
[2024-10-18T23:57:04.098+0000] {subprocess.py:93} INFO - 
[2024-10-18T23:57:04.098+0000] {subprocess.py:93} INFO - If provided paths are partition directories, please set "basePath" in the options of the data source to specify the root directory of the table. If there are multiple root directories, please load them separately and then union them.
[2024-10-18T23:57:04.099+0000] {subprocess.py:93} INFO - 	at scala.Predef$.assert(Predef.scala:223)
[2024-10-18T23:57:04.099+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:178)
[2024-10-18T23:57:04.099+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:110)
[2024-10-18T23:57:04.099+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.inferPartitioning(PartitioningAwareFileIndex.scala:201)
[2024-10-18T23:57:04.099+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.partitionSpec(InMemoryFileIndex.scala:75)
[2024-10-18T23:57:04.100+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.partitionSchema(PartitioningAwareFileIndex.scala:51)
[2024-10-18T23:57:04.100+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:167)
[2024-10-18T23:57:04.100+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
[2024-10-18T23:57:04.100+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
[2024-10-18T23:57:04.100+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
[2024-10-18T23:57:04.101+0000] {subprocess.py:93} INFO - 	at scala.Option.getOrElse(Option.scala:189)
[2024-10-18T23:57:04.101+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
[2024-10-18T23:57:04.101+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)
[2024-10-18T23:57:04.101+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2024-10-18T23:57:04.101+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2024-10-18T23:57:04.102+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2024-10-18T23:57:04.102+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2024-10-18T23:57:04.102+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2024-10-18T23:57:04.102+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2024-10-18T23:57:04.102+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2024-10-18T23:57:04.103+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2024-10-18T23:57:04.103+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2024-10-18T23:57:04.103+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2024-10-18T23:57:04.103+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2024-10-18T23:57:04.103+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Thread.java:840)
[2024-10-18T23:57:04.104+0000] {subprocess.py:93} INFO - 
[2024-10-18T23:57:04.505+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2024-10-18T23:57:04.514+0000] {taskinstance.py:1768} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/usr/local/lib/python3.9/site-packages/airflow/operators/bash.py", line 196, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2024-10-18T23:57:04.516+0000] {taskinstance.py:1318} INFO - Marking task as FAILED. dag_id=ambev_breweries_pipeline, task_id=run_gold, execution_date=20241017T090000, start_date=20241018T235659, end_date=20241018T235704
[2024-10-18T23:57:04.525+0000] {standard_task_runner.py:100} ERROR - Failed to execute job 7 for task run_gold (Bash command failed. The command returned a non-zero exit code 1.; 19147)
[2024-10-18T23:57:04.551+0000] {local_task_job.py:208} INFO - Task exited with return code 1
[2024-10-18T23:57:04.568+0000] {taskinstance.py:2578} INFO - 0 downstream tasks scheduled from follow-on schedule check
